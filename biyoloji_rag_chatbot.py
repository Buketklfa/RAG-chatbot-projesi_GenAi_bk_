# -*- coding: utf-8 -*-
"""biyoloji_rag_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1voqzbC7duLU3V5--t4AMnxl0wDbAIqMb
"""

!pip install chromadb langchain langchain-community sentence-transformers huggingface_hub datasets google-generativeai python-dotenv tqdm streamlit

from datasets import load_dataset
import re

BIO_KEYWORDS = [
    "biyoloji","hücre","dna","rna","protein","enzim","evrim",
    "mitoz","mayoz","genetik","kalıtım","mikroskop","organik",
    "ekoloji","popülasyon","fotosentez","solunum","bakteri",
    "SNP","gen","hayvan","bitki","canlı","metabolizma"
]

def is_biology(text):
    t = text.lower()
    return any(re.search(rf"\b{kw}\b", t) for kw in BIO_KEYWORDS)

# Load the dataset and then select the 'transcription' column
ds = load_dataset("ysdede/khanacademy-turkish", split="train")
ds = ds.select_columns(['transcription'])

bio_texts = [row["transcription"] for row in ds if row.get("transcription") and is_biology(row["transcription"])]

print("Toplam biyoloji kayıt sayısı:", len(bio_texts))

from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import chromadb

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = splitter.split_text("\n\n".join(bio_texts))

model = SentenceTransformer("sentence-transformers/distiluse-base-multilingual-cased-v2")

client = chromadb.Client()
collection = client.create_collection("biology_chunks")

for i, chunk in enumerate(docs):
    emb = model.encode(chunk, normalize_embeddings=True).tolist()
    collection.add(
        ids=[f"doc_{i}"],
        documents=[chunk],
        embeddings=[emb]
    )

print("Toplam chunk sayısı:", len(docs))