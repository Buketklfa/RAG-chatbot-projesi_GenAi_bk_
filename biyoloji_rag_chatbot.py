# -*- coding: utf-8 -*-
"""biyoloji_rag_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WUlHP932_7hyGG-cNjJ_Sc7-tOO2WIIE
"""

# # ğŸ§¬ Khan Academy TÃ¼rkÃ§e Biyoloji RAG Chatbot
#
# Bu notebook, RAG (Retrieval Augmented Generation) mimarisini kullanarak Khan Academy'nin TÃ¼rkÃ§e biyoloji iÃ§eriklerinden filtrelenmiÅŸ verilerle bir chatbot oluÅŸturur.
#
# **Teknik Ã–zellikler:**
# - **Generation Model:** Gemini 2.5 Flash
# - **Embedding Model:** sentence-transformers/distiluse-base-multilingual-cased-v2 (Hugging Face)
# - **Vector DB:** Chroma
# - **RAG Framework:** LangChain
# - **Data Source:** ysdede/khanacademy-turkish
# - **Hata DÃ¼zeltme:** Dinamik sÃ¼tun adÄ± tespiti ve bellek yÃ¶netimi (GPU kullanÄ±mÄ± Ã¶nerilir).

# ==============================================================================
# BÃ–LÃœM 1: KURULUM VE API ANAHTARI AYARLARI
# ==============================================================================

print("1/4: KÃ¼tÃ¼phane kurulumlarÄ± ve API anahtarÄ± ayarlanÄ±yor...")

# BaÄŸÄ±mlÄ±lÄ±k sorunlarÄ±nÄ± (Ã¶zellikle torch/datasets[audio]) gidermek iÃ§in zorunlu kurulumlar
!pip install -q torch
!pip install -q datasets[audio] langchain chromadb sentence-transformers langchain-huggingface google-genai langchain-google-genai langchain-community

# Gerekli kÃ¼tÃ¼phane iÃ§e aktarÄ±mlarÄ±
import os
import sys
import shutil
import gc # Bellek temizliÄŸi iÃ§in
from getpass import getpass
from datasets import load_dataset
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

# --- SABÄ°TLER ---
LLM_MODEL_NAME = "gemini-2.5-flash"
EMBEDDING_MODEL_NAME = "sentence-transformers/distiluse-base-multilingual-cased-v2"
CHROMA_DB_DIR = "./chroma_db_biyoloji_st"
DATASET_NAME = "ysdede/khanacademy-turkish"
BIYOLOJI_KEYWORDS = ["biyoloji", "hÃ¼cre", "DNA", "RNA", "genetik", "evrim", "fotosentez", "solunum", "mitoz", "mayoz", "protein", "enzim", "canlÄ±", "organ"]
MAX_DOCUMENTS = 100 # Bellek sorununu hafifletmek iÃ§in geÃ§ici Ã¼st sÄ±nÄ±r. Ä°stenirse kaldÄ±rÄ±labilir.

# --- API AnahtarÄ±nÄ± Ayarlama ---
if "GEMINI_API_KEY" not in os.environ:
    print("\n--- API ANAHTARI GEREKLÄ° ---")
    api_key = getpass("LÃ¼tfen Gemini API AnahtarÄ±nÄ±zÄ± buraya yapÄ±ÅŸtÄ±rÄ±p ENTER'a basÄ±n: ")
    os.environ["GEMINI_API_KEY"] = api_key

if not os.getenv("GEMINI_API_KEY"):
    print("âŒ HATA: API AnahtarÄ± yÃ¼klenemedi. Program sonlandÄ±rÄ±lÄ±yor.")
    sys.exit(1)

print("âœ… Kurulumlar ve API AnahtarÄ± HazÄ±r.")


# ==============================================================================
# BÃ–LÃœM 2: VERÄ° YÃœKLEME, SÃœTUN TESPÄ°TÄ° VE FÄ°LTRELEME (Dinamik DÃ¼zeltme)
# ==============================================================================

def get_text_column_name(dataset_name):
    """Veri setindeki metin iÃ§eriÄŸini iÃ§eren sÃ¼tun adÄ±nÄ± dinamik olarak tespit eder."""
    print("ğŸ“¢ Veri seti Ã¶zelliklerini (sÃ¼tun adlarÄ±nÄ±) kontrol ediliyor...")
    try:
        # ÅemayÄ± gÃ¶rmek iÃ§in veri setinin bir kÄ±smÄ±nÄ± yÃ¼kle
        dataset_info = load_dataset(dataset_name, split="train", streaming=False)
        available_columns = list(dataset_info.features.keys())

        print(f"   -> Mevcut sÃ¼tunlar: {available_columns}")

        # YaygÄ±n sÃ¼tun adlarÄ±nÄ± kontrol et:
        if 'content' in available_columns:
            return 'content'
        elif 'text' in available_columns:
            return 'text'
        elif 'transcript' in available_columns:
            return 'transcript'
        elif 'transcription' in available_columns: # Added 'transcription'
            return 'transcription'
        else:
            print("\nâŒ HATA: Metin iÃ§eriÄŸi iÃ§in uygun sÃ¼tun ('content', 'text', 'transcript', 'transcription') bulunamadÄ±.")
            sys.exit(1)

    except Exception as e:
        print(f"\nâŒ Kritik Hata: Veri seti Ã¶zellikleri yÃ¼klenemedi: {e}")
        sys.exit(1)

def filter_biyoloji(example, text_column_name):
    """Verilen sÃ¼tun adÄ±na gÃ¶re biyoloji anahtar kelimelerini filtreler."""
    text_content = example.get(text_column_name, '')
    if not text_content:
        return False
    return any(keyword in str(text_content).lower() for keyword in BIYOLOJI_KEYWORDS)

def load_and_prepare_data():
    """Veri setini yÃ¼kler, doÄŸru sÃ¼tun adÄ± ile filtreler ve Document listesi dÃ¶ndÃ¼rÃ¼r."""
    print(f"\n2/4: {DATASET_NAME} veri seti yÃ¼kleniyor ve filtreleniyor...")

    # Dinamik sÃ¼tun adÄ±nÄ± tespit et
    TEXT_COLUMN_NAME = get_text_column_name(DATASET_NAME)
    print(f"âœ… Filtreleme iÃ§in '{TEXT_COLUMN_NAME}' sÃ¼tunu belirlendi.")

    # Veriyi streaming modunda yÃ¼kleme
    dataset = load_dataset(DATASET_NAME, split="train", streaming=True)

    documents = []
    biyoloji_kayit_sayisi = 0

    print(f"   -> Filtreleme BaÅŸlandÄ±. (Maksimum {MAX_DOCUMENTS} kayÄ±t alÄ±nacak)")

    # Veri akÄ±ÅŸÄ±nÄ± tÃ¼keterek filtreleme yapÄ±yoruz.
    for item in dataset:
        if biyoloji_kayit_sayisi >= MAX_DOCUMENTS:
            print(f"   -> Maksimum kayÄ±t sÄ±nÄ±rÄ±na ({MAX_DOCUMENTS}) ulaÅŸÄ±ldÄ±. Ä°ÅŸlem durduruluyor.")
            break

        if filter_biyoloji(item, TEXT_COLUMN_NAME):
            content = item.get(TEXT_COLUMN_NAME)
            if content:
                documents.append(
                    Document(
                        page_content=str(content),
                        metadata={"source": "Khan Academy Biyoloji"}
                    )
                )
                biyoloji_kayit_sayisi += 1
                if biyoloji_kayit_sayisi % 100 == 0: # Changed print frequency
                    print(f"   ... {biyoloji_kayit_sayisi} biyoloji kaydÄ± iÅŸlendi.")

    print(f"\nâœ… Filtreleme tamamlandÄ±. Toplam biyoloji belgesi: {len(documents)}")
    return documents


# ==============================================================================
# BÃ–LÃœM 3: CHUNKING, EMBEDDING VE VECTOR DB OLUÅTURMA
# ==============================================================================

def create_vector_store(documents):
    """Veri parÃ§acÄ±klarÄ±nÄ± oluÅŸturur ve ChromaDB'de vektÃ¶rleÅŸtirir."""
    print(f"\n3/4: Chunking ve VektÃ¶r VeritabanÄ± oluÅŸturuluyor...")

    # 1. Metin BÃ¶lme (RecursiveCharacterTextSplitter)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    chunks = text_splitter.split_documents(documents)
    print(f"   -> Toplam {len(documents)} belgeden {len(chunks)} parÃ§a oluÅŸturuldu.")

    # 2. Embedding Modeli (Sentence Transformer)
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
    )

    # 3. Vector Store (ChromaDB) OluÅŸturma
    if os.path.exists(CHROMA_DB_DIR):
        shutil.rmtree(CHROMA_DB_DIR)

    print("   -> VektÃ¶rleÅŸtirme iÅŸlemi baÅŸlÄ±yor. (GPU tavsiye edilir)")
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=CHROMA_DB_DIR
    )
    vectorstore.persist()
    print(f"âœ… VektÃ¶r VeritabanÄ± baÅŸarÄ±yla oluÅŸturuldu.")

    # Bellek YÃ¶netimi: BÃ¼yÃ¼k listeleri siliyoruz.
    if 'documents' in locals():
        del documents
    if 'chunks' in locals():
        del chunks
    gc.collect()
    print("ğŸ§¹ Bellek temizliÄŸi yapÄ±ldÄ±.")
    return vectorstore


# ==============================================================================
# BÃ–LÃœM 4: RAG ZÄ°NCÄ°RÄ° VE TEST
# ==============================================================================

def create_rag_chain(vectorstore):
    """LLM'i Prompt Template ile birleÅŸtirerek RetrievalQA zincirini oluÅŸturur."""
    print("\n4/4: RAG Zinciri (RetrievalQA) oluÅŸturuluyor...")

    llm = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME, temperature=0.2)

    # Prompt Åablonu: Modelin rolÃ¼nÃ¼ ve kÄ±sÄ±tlamasÄ±nÄ± tanÄ±mlar.
    template = """Sen, yalnÄ±zca saÄŸlanan baÄŸ baÄŸlamdaki Biyoloji sorularÄ±nÄ± yanÄ±tlamak Ã¼zere tasarlanmÄ±ÅŸ, Ã§ok bilgili bir uzmansÄ±n.
EÄŸer soru BÄ°YOLOJÄ° ile ilgili deÄŸilse veya cevabÄ± saÄŸlanan baÄŸ baÄŸlamda bulamÄ±yorsan, kibarca 'ÃœzgÃ¼nÃ¼m, sadece Khan Academy'den filtrelenmiÅŸ biyoloji sorularÄ±na ve eldeki bilgilere dayanarak cevap verebilirim.' diye yanÄ±tla.
CevaplarÄ±nÄ± her zaman TÃ¼rkÃ§e ve anlaÅŸÄ±lÄ±r bir dille ver.

BaÄŸ baÄŸlam:
{context}

Soru: {question}
Cevap:"""

    RAG_PROMPT_TEMPLATE = PromptTemplate(
        input_variables=["context", "question"],
        template=template,
    )

    # Retriever: En benzer 3 parÃ§ayÄ± Ã§eker.
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": RAG_PROMPT_TEMPLATE},
        return_source_documents=True
    )

    print(f"âœ… RAG Zinciri ({LLM_MODEL_NAME}) hazÄ±r.")
    return qa_chain


def run_tests(qa_chain):
    """Chatbot'u denemek iÃ§in Ã¶rnek sorularÄ± Ã§alÄ±ÅŸtÄ±rÄ±r."""

    def ask_biyoloji_chatbot(question):
        print(f"\n" + "â€”"*70)
        print(f">> SORU: {question}")

        result = qa_chain.invoke({"query": question})

        sources = set([doc.metadata.get('source', 'Bilinmeyen Kaynak') for doc in result['source_documents']])

        print(f"\n>> CHATBOT CEVABI:\n{result['result']}")
        print(f"\n>> KULLANILAN KAYNAKLAR: {', '.join(sources)}")
        print("â€”"*70)
        return result

    print("\n" + "="*70)
    print("ğŸ§¬ CHATBOT DENEME TESTLERÄ° BAÅLIYOR")
    print("="*70)

    # Test 1: Ä°stek Sorusu (DNA nedir?)
    ask_biyoloji_chatbot("DNA nedir?")

    # Test 2: Kompleks Biyoloji Sorusu
    ask_biyoloji_chatbot("Mitokondri hangi iÅŸlevi gÃ¶rÃ¼r ve neden hÃ¼crenin gÃ¼Ã§ santrali olarak adlandÄ±rÄ±lÄ±r?")

    # Test 3: Biyoloji DÄ±ÅŸÄ± Soru (KÄ±sÄ±tlama KontrolÃ¼)
    ask_biyoloji_chatbot("Python ile yapay zeka nasÄ±l yapÄ±lÄ±r?")

    print("âœ… TÃ¼m testler tamamlandÄ±. Chatbot kullanÄ±ma hazÄ±rdÄ±r.")


# ==============================================================================
# ANA Ã‡ALIÅMA BLOÄU
# ==============================================================================

if __name__ == "__main__":

    try:
        # 1. Veri setini yÃ¼kle ve hazÄ±rla
        documents = load_and_prepare_data()

        if not documents:
            print("\nâŒ HATA: Biyoloji iÃ§eriÄŸi bulunamadÄ±. Program sonlandÄ±rÄ±lÄ±yor.")
            sys.exit(1)

        # 2. VektÃ¶r veritabanÄ±nÄ± oluÅŸtur
        vectorstore = create_vector_store(documents)

        # 3. RAG zincirini oluÅŸtur
        qa_chain = create_rag_chain(vectorstore)

        # 4. Testleri Ã§alÄ±ÅŸtÄ±r
        run_tests(qa_chain)

    except Exception as e:
        print(f"\nKRÄ°TÄ°K HATA: Proje Ã§alÄ±ÅŸÄ±rken beklenmeyen bir hata oluÅŸtu: {e}")
        # ProgramÄ± sonlandÄ±r.